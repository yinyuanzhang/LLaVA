1. jinxin2 上进行代码debug


<<<<<<< HEAD

=======
>>>>>>> c64228c1cc0ee40c11502a097c56ab290291b2dc
需要fix:
1. 代码校验
模型加载 done
vit-transformer 编码&padding
初步形成分割符 done
模态融合 undo 




motivation记录：
1. 我们的方法为什么不用cross-attention来解决背景和目标的关系问题，而是采用微调的方式实现？




模型训练加速：
1. yolo_model = YOLO('./checkpoints/yolov/yolov8l-seg.pt').to('cpu').eval()  改为 .gpu 实现
2. 



命名:
1. llava-v1.5-7b-task-lora
2. llava-v1.5-7b-task-lora-



明天贯穿全部的test数据集






--- model_vqa_science



减少显存占用问题：
方式1. 将open-clip模型保存至 huggingface，每次调用时直接加载即可。
方式2. 




问题记录：
基于 llava-1.5v-7b 进行微调后，模型的输出为空，且模型infer速度极慢

潜在可能：
1. base_model llava-1.5v-7b 与 vicuna-7b-v1.5不同 (初测与此无关)
2. 基于task lora，mm_project未加载
3. 模型被训坏了？？(稍微训一下就过拟合了？？训练集上无问题) 
4. 新加的token把模型训练出问题了？

尝试：
- debug 参数加载




1. 