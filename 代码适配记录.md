1. jinxin2 上进行代码debug


<<<<<<< HEAD

=======
>>>>>>> c64228c1cc0ee40c11502a097c56ab290291b2dc
需要fix:
1. 代码校验
模型加载 done
vit-transformer 编码&padding
初步形成分割符 done
模态融合 undo 




motivation记录：
1. 我们的方法为什么不用cross-attention来解决背景和目标的关系问题，而是采用微调的方式实现？




模型训练加速：
1. yolo_model = YOLO('./checkpoints/yolov/yolov8l-seg.pt').to('cpu').eval()  改为 .gpu 实现
2. 



命名:
1. llava-v1.5-7b-task-lora
2. llava-v1.5-7b-task-lora-



明天贯穿全部的test数据集






--- model_vqa_science



减少显存占用问题：
方式1. 将open-clip模型保存至 huggingface，每次调用时直接加载即可。
方式2. 