1. vision_tower.imageProcessor 需要重新修改,直接影响到 LazySupervisedDataset 中的 def __getitem__(self, i) 函数
    - image.shape [3,336,336]
    - batch['image'].shape [16,3,336,336]
    - patch_embeds = patch_embeds.flatten(2).transpose(1, 2) 不更改位置编码，直接只修正patch_embedding

11点前实现 参数传递 - model更改

2. 模拟 目标检测工具 实现 imageProcessor的输入格式      imageProcessor无需更改
3. 考虑 vision_tower.vision_tower 是否需要更改      vision_tower需要更改其patch_embedding部分
------------- 明天上午完成
4. 如何查看model的参数是否参与微调，打印出来？然后寻找合适地方和策略通过参数控制我们的 参数是否参与微调？
5. 寻找数据集  vqa专用
6. 寻找 现有的目标检测工具
6. 如何验证微调后的模型实验结果
7. 改为不用lora

--------------

下午：
1. 调用现有工具生成mask实例(只需要确认image的数据格式即可)  done,未验证。

晚上：
1. token拼接
- load_model 的问题
- 确认image_features、background_features、object_features的输出格式
- 可以打印模型，


- image_features.shape [16, 576, 4096]
- background_features.shape 
- object_features.shape 



- 所有参数放置在同一个device上，还未解决



1. mask现在不能用，原因是mask的结果无法分清楚哪个是目标，哪个是背景。
因此，两种方案：
1. 按照多目标检测，将所有目标均返回
2. 按照置信度最高的单目标进行检测。







rotuation_base









2. 参数微调

---  10点前完成

--- 我们应该微调哪个模型？？

--- 数据集如何修改为ocr_vqa进行验证


3. 尝试vqa数据集的效果。

--- 不微调，直接运行结果看效果。





待办：
1. 验证切割模型是否有效
2. 不微调任何参数，直接load过去的模型的效果，即baseline比较结果。
3. self.background_projector 和 self.object_projector 目前统一使用的是 mm_projector
4. 测试 yolov8n.pt 和 yolov8l.pt 






1. 多模态的数据集，用的哪些量化方法
2. 只更改多模态的llm，数据集切换后其量化后的结果，异常值阈值设为0精度应该比较高





fixed:
1. yolov8l 模型 目标检测 & 图像切割 & 输入shape为24的倍数，而llava要求的图像输入为 336*336
    - 336*336 padding为 352*352 
    - 目标检测result包含 boxex，boxex的矩阵均作为前置
    - patch分离

2. 背景和目标token不一致
    - 图片 -》 卷机 
    - 图片 -> 同一个
    - 576token -> embedding -> postion -> 




每张图片的 backgroung_token和object_token 数目不一致问题：
1. batch_size 为1:相当于只能基于 batch_size=1进行训练
2. 训练时padding: 对于embedding之后的结果，同一个batch当中，取max_token进行padding，padding之后再进




多模态大模型的背景cache：
ViT 原本是在大规模的完整的图片上训练出来的模型，ViT是能够表征一整张图片的语义信息的。
现在，想通过ViT、映射层分别对图片的一部分(e.g., 背景/目标)进行编码，背景token + 目标token + 文本token一起拼接送入到llm中。
key：背景  value: 背景通过ViT和映射层之后的编码token
如果来一张新的图片，将其拆分为背景和目标，若cache命中，则直接复用背景的token value。

上述过程，我们假设背景和目标能精准分割；假设cache能命中。
为实现上述目标，我们是否需要对模型进行微调？ 如果微调，其目的和pipeline是什么？



2. 两阶段训练法
​阶段一（独立预对齐）​：
背景ViT + 映射层：仅用背景数据训练。
目标ViT + 映射层：仅用目标数据训练。
​阶段二（联合微调）​：
冻结背景/目标模型参数，仅训练LLM处理拼接输入。
​优点：背景与目标编码的独立性得以保留，LLM学习融合策略。


1. ​改造COCO/Flickr30k：利用实例分割与NLP工具分离描述。
2. ​半自动标注工具链：SAM+BLIP2/GPT-4生成精准描述。
​3. 组合多数据集：如COCO（目标）+ ADE20K（场景）增强背景多样性。



vit与映射层作为一个整体，用于图文匹配。







逻辑梳理：
图片 -> vit.patch_embedding -> image_embedding, index -> 重组为 (background_embedding_padding, index) &  (object_embedding_padding, index)  -> vit.transformer
-> mm_projector[如何分辨使用的是 共享model还是独立的model？]
根据index完成裁剪background_embedding_tokens -> (background_embedding_tokens, object_embedding_tokens)



实现1：只替换embedding，embedding之后如何 padding，该怎么处理？
如果能直接返回两个结果，怪不怪？ -> MyCLIPVisionTransformer 可以定义self.background_encoder = CLIPEncoder(config)、self.object_encoder = CLIPEncoder(config)



1. 确认下 CLIPVisionTransformer 的逻辑，考虑如何进行更改，同步过一下 clip_encoder.py中的image_forward_outs
2. 确认 MyCLIPVisionTransformer 的返回值，最好是能直接获取 输入到 mm.projector 的feature值 【目前的结果返回是 image_forward_out，需要同步更改clip_encoder.py的处理
3. 调用处重新修改逻辑

-----  下午5点前，确认返回的 background_features 和 object_features 符合所需。  
done，但是未验证 & 优化整体逻辑(args、 self传参等)



两个任务：
1. 等待验证结果无误
2. 规划整体参数设计 & 不影响原代码运行的情况下 
- 理清楚 model参数、self参数、arg参数
- 何时定义model，何时load_model / forward
    - build model和load model在这里是一起实现的，构建的时候也完成了模型的初始化
    - forward 是在 Train_model过程 中 prepare_inputs_labels_for_multimodal 实现的
- 问题1: def prepare_inputs_labels_for_multimodal 中没有参数arg，只有self(这里的self代表什么？)，无法将 image_cache 传递进去
- 问题2: initialize_vision_modules 时调用了 vision_tower = build_vision_tower(model_args)
    方式1: 不动原来的模型，即保留vision_tower的定义和初始化，forward中不包含   【先实现这种】
    方式2: vision_tower的定义和初始化替换为 background



验证效果：
- 完全可以通过args.image_cache 来控制
- 保证 background_feature 和 object_feature 的特征数据对齐


todo:
1. 新的模型的参数加载
2. embedding处理(576 tokens -> 577 tokens)








待办：
1. 评估复现
2. embedding分开验证结果校验
3. image_feature的处理




验证篇：
1. 现有的llava_1.5v_7b模型在ocr_vqa的结果跑一遍  【晚上目标】
2. 冻结所有参数、不改变输入顺序(background 和 object拼接)后的结果
3. 冻结所有参数、改变输入顺序(强行按照 image在前，text在后)的结果
4. 微调llm的最后几层、改变输入顺序(强行按照 image在前，text在后)的结果




3.5 上午:
1. 
2. 






1. 复现原有的效果   done
2. 看原有流程如何实施的    
    1. 未找到 mm_projector 在哪里加载的参数
    2. visual_tower 为什么一开始没有加载参数
3. 新的流程 如何加载参数
    1. image_cache 为 false，加载mm_projector的参数，验证效果是否未改变
    2. 加载 visual_tower 和 mm_projector的参数
4. 好好复盘 position_embedding，考虑如何使用绝对/相对position_embedding

_load_state_dict_into_model


不微调直接复用现有的编码结果 
------     
明天：微调mm_projector



1. 重新拆解思路，考虑位置编码采用现有的结构，再尝试下看到底哪里出问题了。
- 


尚未弄清楚的：
1. 图片和文本的顺序 (在现有的llava代码中)
2. mm_projector 是在哪里加载的参数







待办：
1. 走完llava-mini的流程，边走边思考两个问题：
- fusion_model 是在哪里定义的，怎么实现的，如何搬到我们这边
- 模型微调是在哪里实现的，如何应用到我们这边
- 整体架构是在原有的llava上增加了哪些，对我们有什么启示？
- 如何上传其模型至 huggingface



1. 补充model.prefusion_layers
    for layer in self.get_model().prefusion_layers:
        x = layer(x,attention_mask=attention_mask,position_ids=position_ids)[0]

    融合特征的位置编码是为了mask，掩码；而不是引入位置编码

2. 确认各部分的参数设置




1. 模型定义 - mask是如何使用的  （batch_）
    - from_pretrained 的具体实现逻辑
2. 模型参数加载 - 怎么初始化的  
3. 设置模型是否可微调  (vit冻结，其他都是可调的)
4. 数据集下载
5. 模型保存



1. 推理阶段检测一下
2. 理清楚训练阶段到底哪里没有和推理阶段对齐，为什么模型参数未加载
3. 务必移动到相同设备！！



1. 推理阶段可行，但是训练发现无法work，why？
    - 先看一下推理的self.config，再看一下训练的self.config
2. 推理阶段参数也能加载，但是 训练阶段无法work，why？






问题梳理：
1. 模型参数加载不对，比如key、value键值对未对齐
2. 模型预融合模块未实现数据对齐


重新拼接有效token
直接进行裁剪即可，留下的都是有效token，通过object_valid 进行valid token的选择






待办：
1. 原始图像
2. 原始的 combined_mask的图像
3. llava 处理后的336*336的图像
4. combined_mask 处理成 336*336的图像
5. patch_mask 处理后的图像

直接在金老师服务器上运行 lora版本，观察loss变化

6. Transformer encoder阶段的attention_mask 





表征的更好后我们添加 epoch = 2

今晚最好跑起来，同时设置epoch为2，保存两轮结果。 
lora版本：
1. epoch为1时，保存一版  -> 复制后进行infer验证结果
2. epoch为2时，再保存最终结果 -> 主要是观察epoch多1轮是否有利于进一步降低loss


3. 不添加模块预融合层，其训练效果 (条件判断是否添加模态预融合模块)



下午解决目标检测，晚上解决模态预融合




- 先搞定mask全流程，即不用在意mask的质量，保证能运行
    - 传输mask，在 CLIP-transformer处进行处理





1. 训练和推理时不同
- load_prefusion
- attention_mask

2. epoch设置为1和2 / 不接入模态预融合层

3. 自动化/配置化实施



逻辑1:确认epoch是否有用
逻辑2:检查目标检测、attention_mask是否有用
逻辑3:确认模态预融合是否有用
逻辑4:确认是否时lora的原因


辅助：
1. 看ai领域论文
2. 看llm-cache领域论文


去除模态预融合 控制


1. 你为啥不直接基于llava的结果进行微调





问题排除：
1. no graduite问题
2. 



研究思路：
1. 对于多模态大模型来说，比如llava，常见的场景是其image token(比如576)过大，导致模型的TTFT时间较长；
2. 如果尝试对image token进行缓存，但是由于llm本身要求prefixed-cache，即前置token相同的情况下才可使用缓存；
3. 然而，对于个性化场景，图片的相似度非常高，比如对于两张闹钟图片，其手机背景都相似甚至几乎相同，区别是其目标的时间可能存在差异，比如是9点或者10点
4. 因此，我们考虑对于一张图片来说，缓存其背景token
5. 因此，当一张新的图片到来时，我们再去匹配是否有相似的背景token，如果有则直接使用背景token，只需要计算目标token和text token即可
6. 为了适应现有的多模态模型，输入的input将改为 背景token + 目标token + 文本text token
7. 为了实现背景token和目标token不互相影响的目的，即背景token可以直接拿去复用的目的，我们需要让背景部分图片和目标部分图片分别进行编码
8. 具体表现为：一张图片，经过yolo目标检测工具将图片分为输入的背景token 和 输入的目标token
9. 背景token 和 目标token 分别经过vit、projector，生成背景token、目标token，最后一起经过llm

问题是：
1. 由于我们需要对比的baseline为llava，因此我理解为我们应该是需要进行微调或者预训练？
2. 上述模型范式和llava相比存在什么问题(llava是对一整张图片进行处理)


我们的模型：
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(


baseline-llava模型：



1. 需要确认你的代码是严谨的、流畅的
2. 需要使用渐进的方式实现


可参考：
Video-LLaVA
LLaVA-Mini


- 采用语义更清晰的分割符，如[SEP_BG_OBJ] / [BG_STARTJ] [BG_ENDJ] [OBJ_STARTJ] [OBJ_ENDJ] 
- 为什么 train使用的是一个token，推理阶段没有新增tokenizer呢？


input prompt:
"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\nWhat credit card company is on the banner in the background?\nAnswer the question using a single word or phrase. ASSISTANT:"

<image> 的占位符是-200



目前代码问题：
1. 为什么open-clip的模型需要从local导入  done
    模型参数加载无误

2. forward & 模型定义是否无误

3. 有没有可能是模型微调过度，只微调llm后面3层就可以了？





渐进式微调顺序：
1. 代码校验无误-先确认代码没问题
2. llava-1.5v微调之后的模型，不做任何微调，直接输出最终的结果 
3. 基于llava-1.5v现成的模型，对我们的模型(同上，除了添加分割符，无任何变化)进行微调
4. 预训练：同步预训练阶段，预训练模态融合 + 投影层
5. 微调：




1. 代码无误
2. 更改现有 token切割标志符




1. 确认分割编码使用是否正确 
done, 但留个隐含问题：1. 目前的token是随机初始化的，并没有明确的语义说明这是背景，这是目标，我们能否训练的好；2. 没有针对检查不成功的情况下，是否需要移除该标志符


2. 再过一遍过vit部分的编码
3. train阶段完整过一遍 vit和聚合

4. 再加一个模态融合部分
5. 测试一下 lora 显存占用最少多少 24GB是否支持


夜里开始运行，基于完整的llava模型，跑一个先做 模态融合 + 映射层 微调的模型(速度应该较快) - 该模型的结果在jinxin2上进行测试验证
再跑一个全部微调的模型



1. jinxin 尝试运行 finetune  基于llava微调好的模型 -llava -mm_project   今晚
2. 实例分析 - 目标检测扩大范围(重叠) - 试试效果  【先看patch-size，再试试重叠】
